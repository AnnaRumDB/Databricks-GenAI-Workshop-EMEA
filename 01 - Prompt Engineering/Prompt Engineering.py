# Databricks notebook source
dbutils.widgets.text("catalog_name","main")

# COMMAND ----------

# MAGIC %run ./init/config $catalog_name=$catalog_name

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC # Prompt Engineering
# MAGIC
# MAGIC Prompt engineering is a critical aspect of interacting effectively with Large Language Models (LLMs). It involves crafting prompts that can be appended to user inputs that guide the model to generate the most relevant and accurate outputs. This skill is valuable for several reasons:
# MAGIC
# MAGIC 1. **Precision and Relevance**: Well-engineered prompts help the model understand the context and specificity of the query, leading to more precise and relevant responses.
# MAGIC 2. **Efficiency**: Effective prompts can reduce the number tokens generated by the model, while still maintaining accuracy/correctness. This saves time and computational resources.
# MAGIC 3. **Creative and Complex Tasks**: For tasks that require creativity or complex problem-solving, carefully designed prompts can significantly improve the quality of the model's output.

# COMMAND ----------

# MAGIC %md
# MAGIC %md
# MAGIC <img src="https://daxg39y63pxwu.cloudfront.net/images/blog/langchain/LangChain.webp" alt="LangChain" width="700"/>
# MAGIC
# MAGIC
# MAGIC ## LangChain 
# MAGIC LangChain is a framework designed to simplify the creation of applications using large It enables applications that:
# MAGIC     - Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)
# MAGIC     - Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) language models.

# COMMAND ----------

import os
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import OpenAI
import openai

open_ai_key = dbutils.secrets.get(scope = "doan-scope", key = "openai-key2")
os.environ["OPENAI_API_KEY"] = open_ai_key
openai.api_key = os.environ["OPENAI_API_KEY"]

# COMMAND ----------

# DBTITLE 1,Ask OpenAI Function for Comparisons
def ask_open_ai(input_string, model, max_tokens, temperature):
  """
  Calls open AI with model and desired input string to return a single response. 

  Params
  =======
  input_string (str): The string/query being passed to the model
  model (str): The name of the OpenAI model
  max_tokens (int): Max number of tokens in response
  temperature (float): The temperature value (higher temperature means more diverse responses)
  """
  response = openai.Completion.create(
        model=model,
        prompt = input_string,
        max_tokens=max_tokens,
        n=1,
        stop=None,
        temperature=temperature,
    )  
  
  answer = response.choices[0].text.strip()
  return answer

# COMMAND ----------

# DBTITLE 1,Start with a Simple Question about Spark Joins
user_question = "How can I speed up my Spark join operation?"

# COMMAND ----------

#select our model
model_name = "text-davinci-003"
#max tokens helps us control the length of the model response, which in turn helps us control inference time
max_tokens = 200
#temperature can be used to toggle reponse diversity
temperature = 0.5

ask_open_ai(user_question, model_name, max_tokens, temperature)

# COMMAND ----------

# DBTITLE 1,Creating a Prompt Template to Append to our Inputs
from langchain import PromptTemplate
from langchain.chains import LLMChain

#now, let's create a prompt template to make our incoming queries databricks-specific
intro_template = """
You are a Databricks support engineer tasked with answering questions about Spark. Include Databricks-relevant information in your response and be as prescriptive as possible. Cite Databricks documentation for your answers
User Question:" {question}"
"""

# COMMAND ----------

# DBTITLE 1,Create an LLM Chain that appends our Template to the User Input
openAI_LLM = OpenAI(model="text-davinci-003")

prompt_template = PromptTemplate(
    input_variables=["question"],
    template=intro_template,
)

openAI_Chain = LLMChain(
    llm=openAI_LLM,
    prompt=prompt_template,
    output_key="Support Response",
    verbose=False
)

openAI_Chain_response = openAI_Chain.run({"question":user_question})
print(openAI_Chain_response)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Zero-Shot Prompting
# MAGIC Don't provide any examples to the model, and just ask the question
# MAGIC
# MAGIC ## Few Shot Prompting
# MAGIC Provide N examples to the model to help guide it's response

# COMMAND ----------

# DBTITLE 1,Helper Function to our LLM Chains
def run_llm_chain(input_string, template_string, model):
  """
  given an input string, template, and model, execute a langchain chain on the input with a given prompt template

  params
  ==========
  input_string (str): the incoming query from a user to be evaluated
  template_string (str): prompt template append or pre-pend to input_string (required for prompt engineering)
  model (langchain model): the name of the model 
  """
  prompt_template = PromptTemplate(
    input_variables=["input_string"],
    template=template_string,
  )
  model_chain = LLMChain(
    llm=model,
    prompt=prompt_template,
    output_key="Response",
    verbose=False
  )

  return model_chain.run({"input_string": input_string})

# COMMAND ----------

# DBTITLE 1,Create our Zero and Few Shot Prompts
zero_shot_template = """For each tweet, describe its sentiment:
                        [Tweet]: {input_string}
                      """

few_shot_template = """For each tweet, describe its sentiment:
                        [Tweet]: "I hate it when my phone battery dies."
                        [Sentiment]: Negative
                        ###
                        [Tweet]: "My day has been üëç"
                        [Sentiment]: Positive
                        ###
                        [Tweet]: "This is the link to the article"
                        [Sentiment]: Neutral
                        ###
                        [Tweet]: {input_string}
                        [Sentiment]:
                      """

# COMMAND ----------

tweet = "My day has been ugh"
zero_shot_response = run_llm_chain(tweet, zero_shot_template, openAI_LLM)
print(zero_shot_response)

# COMMAND ----------

few_shot_response = run_llm_chain(tweet, few_shot_template, openAI_LLM)
print(few_shot_response)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Chain of Thought Prompting
# MAGIC
# MAGIC Chain of thought prompting makes the LLM step through it's decision-making process. This makes it easier to better understand model outputs or identify hallucinations

# COMMAND ----------

apples = """
"Imagine you are at a grocery store and need to buy apples. They are sold in bags of 6 apples each and cost $2 per bag. If you need 20 apples for a recipe, how many bags should you buy and how much will it cost?
"""

ask_open_ai(apples, model_name, max_tokens, temperature)

# COMMAND ----------

chain_of_reasoning_prompt = """
                            For the following question, answer the question, but walk through your line of reasining step by step to arrive at the answer:

                            {input_string}
                            """

cor_response = run_llm_chain(apples, chain_of_reasoning_prompt, openAI_LLM)
print(cor_response)

# COMMAND ----------

# MAGIC %md
# MAGIC # No Hallucinations!
# MAGIC
# MAGIC Finally, you can tell your model not to hallucinate. LLMs typically return a confidence score for their output sequence, so you can explicitly tell the model not to respond if it feels it does not have enough information to answer

# COMMAND ----------

no_hallucinations_prompt = """
                            For the following question, only respond if you have sufficient information to generate a confident answer. If you cannot do so, then simply respond 'Sorry - I don't have enough information to answer that.'

                            Question:
                            {input_string}
                            """

# COMMAND ----------

liquid_cluster = "What is liquid clustering?"
ask_open_ai(liquid_cluster, model_name, max_tokens, temperature)

# COMMAND ----------

run_llm_chain(liquid_cluster, no_hallucinations_prompt, openAI_LLM)

# COMMAND ----------

# MAGIC %md
# MAGIC # Try It Yourself!
# MAGIC Below, we set up a few LangChain prompts

# COMMAND ----------

#when crafting your prompt, remember that you need to add a variable injection (commonly with {}) to the template. See examples above.
#once you have the prompt + variable, add the variable build your PromptTemplate in cell 25 below
your_prompt_template_here = """
                            """

# COMMAND ----------

your_question_here = """
                     """

# COMMAND ----------

ask_open_ai(your_question_here, model_name, max_tokens, temperature)

# COMMAND ----------

#build your own PromptTemplate + LLM chain
your_prompt_template = PromptTemplate(
    #TODO
  )

your_model_chain = LLMChain(
  #TODO
)

# COMMAND ----------

model_chain.run() #TODO: what do we need to pass into run()?

# COMMAND ----------

# MAGIC %md
# MAGIC # Saving our LangChain LLM Chain to MLflow
# MAGIC This should look familiar!

# COMMAND ----------

from mlflow.models import infer_signature

input_str="How can I speed up my Spark joins?"
prediction = openAI_Chain.run(input_str)
input_columns = [
    {"type": "string", "name": input_key} for input_key in openAI_Chain.input_keys
]
signature = infer_signature(input_columns, prediction)

# COMMAND ----------

import mlflow
#Create a new mlflow experiment or get the existing one if already exists.
experiment_name = f"/Users/{current_user}/genai-prompt-engineering-workshop"
mlflow.set_experiment(experiment_name)

#set the name of our model
model_name = "2langchainz"

#get experiment id to pass to the run
experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id
with mlflow.start_run(experiment_id=experiment_id):
  mlflow.langchain.log_model(openAI_Chain, model_name, signature=signature, input_example=input_str)

# COMMAND ----------

import mlflow

#grab our most recent run (which logged the model) using our experiment ID
runs = mlflow.search_runs([experiment_id])
last_run_id = runs.sort_values('start_time', ascending=False).iloc[0].run_id

#grab the model URI that's generated from the run
model_uri = f"runs:/{last_run_id}/{model_name}"

#log the model to catalog.schema.model. The schema name referenced below is generated for you in the init script
catalog = dbutils.widgets.get("catalog_name")
schema = schema_name

#set our registry location to Unity Catalog
mlflow.set_registry_uri("databricks-uc")
mlflow.register_model(
    model_uri=model_uri,
    name=f"{catalog}.{schema}.{model_name}"
)

# COMMAND ----------


